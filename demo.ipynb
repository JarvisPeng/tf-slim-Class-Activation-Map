{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_y(argument):\n",
    "    switcher = {\n",
    "        'c0':[1,0,0,0,0,0,0,0,0,0],\n",
    "        'c1':[0,1,0,0,0,0,0,0,0,0],\n",
    "        'c2':[0,0,1,0,0,0,0,0,0,0],\n",
    "        'c3':[0,0,0,1,0,0,0,0,0,0],\n",
    "        'c4':[0,0,0,0,1,0,0,0,0,0],\n",
    "        'c5':[0,0,0,0,0,1,0,0,0,0],\n",
    "        'c6':[0,0,0,0,0,0,1,0,0,0],\n",
    "        'c7':[0,0,0,0,0,0,0,1,0,0],\n",
    "        'c8':[0,0,0,0,0,0,0,0,1,0],\n",
    "        'c9':[0,0,0,0,0,0,0,0,0,1],\n",
    "    }\n",
    "    return switcher.get(argument, \"nothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader=pd.read_csv('data.csv')\n",
    "y0=[]\n",
    "for i in reader['classname']:\n",
    "    y0.append(to_y(i))\n",
    "data=[]\n",
    "import random\n",
    "sclice = random.sample(range(len(yy)), len(yy))\n",
    "for i in sclice:\n",
    "    data.append([y0[i],'train/'+reader['img'][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_batches = 40\n",
    "batch_size = 512 \n",
    "test_features=[]\n",
    "test_labels=[]\n",
    "for i in range((n_batches+1)*batch_size,len(y0)):\n",
    "    img = cv2.imread(data[i][1])\n",
    "    resized = cv2.resize(img, (299,299), interpolation=cv2.INTER_AREA)\n",
    "    test_features.append(resized)\n",
    "    test_labels.append(data[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    i =tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=[None, *image_shape], name='x')\n",
    "    return i\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    i =tf.placeholder(\n",
    "    tf.float32,\n",
    "    shape=[None, n_classes], name='y')\n",
    "    return i\n",
    "\n",
    "def load_preprocess_training_batch(batch_id, batch_size):\n",
    "    \"\"\"\n",
    "    Load the Preprocessed Training data and return them in batches of <batch_size> or less\n",
    "    \"\"\"\n",
    "    features=[]\n",
    "    labels=[]\n",
    "    for i in range(batch_id*batch_size,batch_id*batch_size+batch_size):\n",
    "        img = cv2.imread(data[i][1])\n",
    "        resized = cv2.resize(img, (299,299), interpolation=cv2.INTER_AREA)\n",
    "        features.append(resized)\n",
    "        labels.append(data[i][0])\n",
    "\n",
    "    # Return the training data in batches of size <batch_size> or less\n",
    "    yield features, labels\n",
    "    \n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch})\n",
    "    valid_acc = session.run(accuracy, feed_dict={\n",
    "        x: test_features,\n",
    "        y: test_labels})\n",
    "    print(loss,end=' ')\n",
    "    print(valid_acc)\n",
    "    return([loss,valid_acc])\n",
    "\n",
    "\n",
    "zhfont1 = matplotlib.font_manager.FontProperties(fname=r\"c:\\windows\\fonts\\simsun.ttc\", size=16)  \n",
    "\n",
    "def _load_label_names():\n",
    "    \"\"\"\n",
    "    Load the label names from file\n",
    "    \"\"\"\n",
    "    return [\n",
    "    u'c0',\n",
    "    u'c1',\n",
    "    u'c2',\n",
    "    u'c3',\n",
    "    u'c4',\n",
    "    u'c5',\n",
    "    u'c6',\n",
    "    u'c7',\n",
    "    u'c8',\n",
    "    u'c9'\n",
    "]\n",
    "\n",
    "\n",
    "def display_image_predictions(features, labels, predictions):\n",
    "    n_classes = 10\n",
    "    label_names = _load_label_names()\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    label_binarizer.fit(range(n_classes))\n",
    "    label_ids = label_binarizer.inverse_transform(np.array(labels))\n",
    "\n",
    "    fig, axies = plt.subplots(nrows=4, ncols=2, figsize=(20,12))\n",
    "    fig.tight_layout()\n",
    "    fig.suptitle('Predictions', fontsize=40, y=1.1)\n",
    "    n_predictions = 3\n",
    "    margin = 0.05\n",
    "    ind = np.arange(n_predictions)\n",
    "    width = (1. - 2. * margin) / n_predictions\n",
    "\n",
    "    for image_i, (feature, label_id, pred_indicies, pred_values) in enumerate(zip(features, label_ids, predictions.indices, predictions.values)):\n",
    "        pred_names = [label_names[pred_i] for pred_i in pred_indicies]\n",
    "        correct_name = label_names[label_id]\n",
    "\n",
    "        axies[image_i][0].imshow(feature)\n",
    "        axies[image_i][0].set_title(correct_name,fontproperties=zhfont1)\n",
    "        axies[image_i][0].set_axis_off()\n",
    "\n",
    "        axies[image_i][1].barh(ind + margin, pred_values[::-1], width)\n",
    "        axies[image_i][1].set_yticks(ind + margin)\n",
    "        axies[image_i][1].set_yticklabels(pred_names[::-1],fontproperties=zhfont1)\n",
    "        axies[image_i][1].set_xticks([0])\n",
    "  \n",
    "def batch_features_labels(features, labels, batch_size):\n",
    "    \"\"\"\n",
    "    Split features and labels into batches\n",
    "    \"\"\"\n",
    "    for start in range(0, len(features), batch_size):\n",
    "        end = min(start + batch_size, len(features))\n",
    "        yield features[start:end], labels[start:end]\n",
    "        \n",
    "\n",
    "def py_returnCAMmap(activation, weights_LR):\n",
    "#     print(activation.shape)\n",
    "#     print(weights_LR.shape)\n",
    "    n_feat, w, h, n= activation.shape\n",
    "    act_vec = np.reshape(activation, [n_feat, w*h])\n",
    "    n_top = weights_LR.shape[0]\n",
    "    out = np.zeros([w, h, n_top])\n",
    "\n",
    "    for t in range(n_top):\n",
    "        weights_vec = np.reshape(weights_LR[t], [1, weights_LR[t].shape[0]])\n",
    "        heatmap_vec = np.dot(weights_vec,act_vec)\n",
    "        heatmap = np.reshape( np.squeeze(heatmap_vec) , [w, h])\n",
    "        out[:,:,t] = heatmap\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def im2double(im):\n",
    "    return cv2.normalize(im.astype('float'), None, 0.0, 1.0, cv2.NORM_MINMAX)\n",
    "\n",
    "\n",
    "def py_map2jpg(imgmap, rang, colorMap):\n",
    "    if rang is None:\n",
    "        rang = [np.min(imgmap), np.max(imgmap)]\n",
    "\n",
    "    heatmap_x = np.round(imgmap*255).astype(np.uint8)\n",
    "\n",
    "    return cv2.applyColorMap(heatmap_x, cv2.COLORMAP_JET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding:UTF-8\n",
    "slim = tf.contrib.slim\n",
    "trunc_normal = lambda stddev: tf.truncated_normal_initializer(0.0, stddev)\n",
    "\n",
    "# 用来生成网络中经常用到的函数的默认参数\n",
    "# 默认参数：卷积的激活函数、权重初始化方式、标准化器等\n",
    "def inception_v3_arg_scope(weight_decay=0.00004,    # L2正则的weight_decay\n",
    "                           stddev=0.1,  # 标准差0.1\n",
    "                           batch_norm_var_collection='moving_vars'):\n",
    "\n",
    "    batch_norm_params = {  # 定义batch normalization参数字典\n",
    "      'decay': 0.9997,  #衰减系数\n",
    "      'epsilon': 0.001,\n",
    "      'updates_collections': tf.GraphKeys.UPDATE_OPS,\n",
    "      'variables_collections': {\n",
    "          'beta': None,\n",
    "          'gamma': None,\n",
    "          'moving_mean': [batch_norm_var_collection],\n",
    "          'moving_variance': [batch_norm_var_collection],\n",
    "      }\n",
    "  }\n",
    "\n",
    "  # silm.arg_scope可以给函数自动赋予某些默认值\n",
    "  # 会对[slim.conv2d, slim.fully_connected]这两个函数的参数自动赋值,\n",
    "  # 使用slim.arg_scope后就不需要每次都重复设置参数了，只需要在有修改时设置\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      weights_regularizer=slim.l2_regularizer(weight_decay)): # 对[slim.conv2d, slim.fully_connected]自动赋值\n",
    "\n",
    "      # 嵌套一个slim.arg_scope对卷积层生成函数slim.conv2d的几个参数赋予默认值\n",
    "        with slim.arg_scope(\n",
    "            [slim.conv2d],\n",
    "            weights_initializer=trunc_normal(stddev), # 权重初始化器\n",
    "            activation_fn=tf.nn.relu, # 激活函数\n",
    "            normalizer_fn=slim.batch_norm, # 标准化器\n",
    "            normalizer_params=batch_norm_params) as sc: # 标准化器的参数设置为前面定义的batch_norm_params\n",
    "            return sc # 最后返回定义好的scope\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 生成V3网络的卷积部分\n",
    "def inception_v3_base(inputs, scope=None):\n",
    "    '''\n",
    "    Args:\n",
    "    inputs：输入的tensor\n",
    "    scope：包含了函数默认参数的环境\n",
    "    '''\n",
    "    end_points = {} # 定义一个字典表保存某些关键节点供之后使用\n",
    "\n",
    "    with tf.variable_scope(scope, 'InceptionV3', [inputs]):\n",
    "        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], # 对三个参数设置默认值\n",
    "                        stride=1, padding='VALID'):\n",
    "\n",
    "      #  因为使用了slim以及slim.arg_scope，我们一行代码就可以定义好一个卷积层\n",
    "      #  相比AlexNet使用好几行代码定义一个卷积层，或是VGGNet中专门写一个函数定义卷积层，都更加方便\n",
    "      #\n",
    "      # 正式定义Inception V3的网络结构。首先是前面的非Inception Module的卷积层\n",
    "      # slim.conv2d函数第一个参数为输入的tensor，第二个是输出的通道数，卷积核尺寸，步长stride，padding模式\n",
    "\n",
    "      #一共有5个卷积层，2个池化层，实现了对图片数据的尺寸压缩，并对图片特征进行了抽象\n",
    "      # 299 x 299 x 3\n",
    "#             print(inputs)\n",
    "            net = slim.conv2d(inputs, 32, [3, 3],\n",
    "                            stride=2, scope='Conv2d_1a_3x3')    # 149 x 149 x 32\n",
    "\n",
    "            net = slim.conv2d(net, 32, [3, 3],\n",
    "                            scope='Conv2d_2a_3x3')      # 147 x 147 x 32\n",
    "\n",
    "            net = slim.conv2d(net, 64, [3, 3], padding='SAME',\n",
    "                            scope='Conv2d_2b_3x3')  # 147 x 147 x 64\n",
    "\n",
    "            net = slim.max_pool2d(net, [3, 3], stride=2,\n",
    "                                scope='MaxPool_3a_3x3')   # 73 x 73 x 64\n",
    "\n",
    "            net = slim.conv2d(net, 80, [1, 1],\n",
    "                            scope='Conv2d_3b_1x1')  # 73 x 73 x 80\n",
    "\n",
    "            net = slim.conv2d(net, 192, [3, 3],\n",
    "                            scope='Conv2d_4a_3x3')  # 71 x 71 x 192\n",
    "\n",
    "            net = slim.max_pool2d(net, [3, 3], stride=2,\n",
    "                                scope='MaxPool_5a_3x3') # 35 x 35 x 192\n",
    "\n",
    "\n",
    "    '''\n",
    "    三个连续的Inception模块组，三个Inception模块组中各自分别有多个Inception Module，这部分是Inception Module V3\n",
    "    的精华所在。每个Inception模块组内部的几个Inception Mdoule结构非常相似，但是存在一些细节的不同\n",
    "    '''\n",
    "    # Inception blocks\n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], # 设置所有模块组的默认参数\n",
    "                        stride=1, padding='SAME'): # 将所有卷积层、最大池化、平均池化层步长都设置为1\n",
    "      # 第一个模块组包含了三个结构类似的Inception Module\n",
    "        '''    \n",
    "        --------------------------------------------------------    \n",
    "        第一个Inception组   一共三个Inception模块\n",
    "        '''\n",
    "        with tf.variable_scope('Mixed_5b'): # 第一个Inception Module名称。Inception Module有四个分支\n",
    "        # 第一个分支64通道的1*1卷积\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1') # 35x35x64\n",
    "        \n",
    "            # 第二个分支48通道1*1卷积，链接一个64通道的5*5卷积\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1') # 35x35x48\n",
    "                branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv2d_0b_5x5') #35x35x64\n",
    "\n",
    "            # 第三个分支64通道1*1卷积,96的3*3,再接一个3*3\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')#35x35x96\n",
    "\n",
    "            # 第四个分支64通道3*3平均池化,32的1*1\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')            \n",
    "                branch_3 = slim.conv2d(branch_3, 32, [1, 1], scope='Conv2d_0b_1x1') #35*35*32\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3) # 将四个分支的输出合并在一起（第三个维度合并，即输出通道上合并）\n",
    "        # 64+64+96+32 = 256\n",
    "        # mixed_1: 35 x 35 x 256.\n",
    "        '''\n",
    "        因为这里所有层步长均为1，并且padding模式为SAME，所以图片尺寸不会缩小，但是通道数增加了。四个分支通道数之和\n",
    "        64+64+96+32=256，最终输出的tensor的图片尺寸为35*35*256\n",
    "        '''\n",
    "\n",
    "        with tf.variable_scope('Mixed_5c'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0b_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv_1_0c_5x5')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "          # 64+64+96+64 = 288\n",
    "          # mixed_2: 35 x 35 x 288.\n",
    "\n",
    "        with tf.variable_scope('Mixed_5d'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 48, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 64, [5, 5], scope='Conv2d_0b_5x5')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 64, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "\n",
    "          # 64+64+96+64 = 288\n",
    "          # mixed_1: 35 x 35 x 288\n",
    "\n",
    "        '''    \n",
    "        第一个Inception组结束  一共三个Inception模块 输出为:35*35*288\n",
    "----------------------------------------------------------------------    \n",
    "        第二个Inception组   共5个Inception模块\n",
    "        '''\n",
    "\n",
    "        with tf.variable_scope('Mixed_6a'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 384, [3, 3], stride=2,\n",
    "                                     padding='VALID', scope='Conv2d_1a_1x1') #17*17*384\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1') #35*35*64\n",
    "                branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='Conv2d_0b_3x3')#35*35*96\n",
    "                branch_1 = slim.conv2d(branch_1, 96, [3, 3], stride=2,\n",
    "                                     padding='VALID', scope='Conv2d_1a_1x1') #17*17*96\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n",
    "                                         scope='MaxPool_1a_3x3') #17*17*288\n",
    "            net = tf.concat([branch_0, branch_1, branch_2], 3) # 输出尺寸定格在17 x 17 x 768\n",
    "        # 384+96+288 = 768\n",
    "        # mixed_3: 17 x 17 x 768.\n",
    "\n",
    "        with tf.variable_scope('Mixed_6b'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 128, [1, 7], scope='Conv2d_0b_1x7') # 串联1*7卷积和7*1卷积合成7*7卷积，减少了参数，减轻了过拟合\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 128, [1, 1], scope='Conv2d_0a_1x1') # 反复将7*7卷积拆分\n",
    "                branch_2 = slim.conv2d(branch_2, 128, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 128, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                branch_2 = slim.conv2d(branch_2, 128, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "        # 192+192+192+192 = 768\n",
    "        # mixed4: 17 x 17 x 768.\n",
    "\n",
    "\n",
    "        with tf.variable_scope('Mixed_6c'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                '''\n",
    "                我们的网络每经过一个inception module，即使输出尺寸不变，但是特征都相当于被重新精炼了一遍，\n",
    "                其中丰富的卷积和非线性化对提升网络性能帮助很大。\n",
    "                '''\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "          # 192+192+192+192 = 768\n",
    "          # mixed_5: 17 x 17 x 768.\n",
    "\n",
    "\n",
    "        with tf.variable_scope('Mixed_6d'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 160, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 160, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 160, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                branch_2 = slim.conv2d(branch_2, 160, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "          # 92+192+192+192 = 768\n",
    "          # mixed_6: 17 x 17 x 768.\n",
    "\n",
    "        with tf.variable_scope('Mixed_6e'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0b_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0c_1x7')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0d_7x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 192, [1, 7], scope='Conv2d_0e_1x7')\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "          # 92+192+192+192 = 768\n",
    "          # mixed_7: 17 x 17 x 768.\n",
    "\n",
    "\n",
    "            '''    \n",
    "            第二个Inception组结束  一共五个Inception模块 输出为:17*17*768\n",
    "    ----------------------------------------------------------------------    \n",
    "            第三个Inception组   共3个Inception模块(带分支)\n",
    "            '''\n",
    "          # 将Mixed_6e存储于end_points中，作为Auxiliary Classifier辅助模型的分类\n",
    "        end_points['Mixed_6e'] = net\n",
    "\n",
    "          # 第三个inception模块组包含了三个inception module\n",
    "\n",
    "        with tf.variable_scope('Mixed_7a'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1')# 17*17*192\n",
    "                branch_0 = slim.conv2d(branch_0, 320, [3, 3], stride=2,\n",
    "                                     padding='VALID', scope='Conv2d_1a_3x3') # 8*8*320\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 192, [1, 1], scope='Conv2d_0a_1x1') #17*17*192\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [1, 7], scope='Conv2d_0b_1x7')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [7, 1], scope='Conv2d_0c_7x1')\n",
    "                branch_1 = slim.conv2d(branch_1, 192, [3, 3], stride=2,\n",
    "                                     padding='VALID', scope='Conv2d_1a_3x3') #8*8*192\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n",
    "                                         scope='MaxPool_1a_3x3')  #8*8*768\n",
    "            net = tf.concat([branch_0, branch_1, branch_2], 3) # 输出图片尺寸被缩小，通道数增加，tensor的总size在持续下降中\n",
    "          # 320+192+768 = 1280\n",
    "          # mixed_8: 8 x 8 x 1280.\n",
    "\n",
    "\n",
    "        with tf.variable_scope('Mixed_7b'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 320, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 384, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = tf.concat([\n",
    "                  slim.conv2d(branch_1, 384, [1, 3], scope='Conv2d_0b_1x3'),\n",
    "                  slim.conv2d(branch_1, 384, [3, 1], scope='Conv2d_0b_3x1')], 3)\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 448, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 384, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                branch_2 = tf.concat([\n",
    "                  slim.conv2d(branch_2, 384, [1, 3], scope='Conv2d_0c_1x3'),\n",
    "                  slim.conv2d(branch_2, 384, [3, 1], scope='Conv2d_0d_3x1')], 3)\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3) # 输出通道数增加到2048\n",
    "          # 320+(384+384)+(384+384)+192 = 2048\n",
    "          # mixed_9: 8 x 8 x 2048.\n",
    "\n",
    "\n",
    "        with tf.variable_scope('Mixed_7c'):\n",
    "            with tf.variable_scope('Branch_0'):\n",
    "                branch_0 = slim.conv2d(net, 320, [1, 1], scope='Conv2d_0a_1x1')\n",
    "            with tf.variable_scope('Branch_1'):\n",
    "                branch_1 = slim.conv2d(net, 384, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_1 = tf.concat([\n",
    "                  slim.conv2d(branch_1, 384, [1, 3], scope='Conv2d_0b_1x3'),\n",
    "                  slim.conv2d(branch_1, 384, [3, 1], scope='Conv2d_0c_3x1')], 3)\n",
    "            with tf.variable_scope('Branch_2'):\n",
    "                branch_2 = slim.conv2d(net, 448, [1, 1], scope='Conv2d_0a_1x1')\n",
    "                branch_2 = slim.conv2d(branch_2, 384, [3, 3], scope='Conv2d_0b_3x3')\n",
    "                branch_2 = tf.concat([\n",
    "                  slim.conv2d(branch_2, 384, [1, 3], scope='Conv2d_0c_1x3'),\n",
    "                  slim.conv2d(branch_2, 384, [3, 1], scope='Conv2d_0d_3x1')], 3)\n",
    "            with tf.variable_scope('Branch_3'):\n",
    "                branch_3 = slim.avg_pool2d(net, [3, 3], scope='AvgPool_0a_3x3')\n",
    "                branch_3 = slim.conv2d(branch_3, 192, [1, 1], scope='Conv2d_0b_1x1')\n",
    "            net = tf.concat([branch_0, branch_1, branch_2, branch_3], 3)\n",
    "          # 320+(384+384)+(384+384)+192 = 2048\n",
    "          # mixed_10: 8 x 8 x 2048.\n",
    "\n",
    "        return net, end_points\n",
    "#Inception V3网络的核心部分，即卷积层部分就完成了\n",
    "\n",
    "\n",
    "\n",
    "# V3最后部分\n",
    "# 全局平均池化、Softmax和Auxiliary Logits\n",
    "def inception_v3(inputs,\n",
    "                 num_classes=10, # 最后需要分类的数量（比赛数据集的种类数）\n",
    "                 is_training=True, # 标志是否为训练过程，只有在训练时Batch normalization和Dropout才会启用\n",
    "                 dropout_keep_prob=0.8, # 节点保留比率\n",
    "                 prediction_fn=slim.softmax, # 最后用来分类的函数\n",
    "                 spatial_squeeze=True, # 参数标志是否对输出进行squeeze操作（去除维度数为1的维度，比如5*3*1转为5*3）\n",
    "                 reuse=None, # 是否对网络和Variable进行重复使用\n",
    "                 scope='InceptionV3'): # 包含函数默认参数的环境\n",
    "    with tf.variable_scope(scope, 'InceptionV3', [inputs, num_classes], # 定义参数默认值\n",
    "                         reuse=reuse) as scope:\n",
    "        with slim.arg_scope([slim.batch_norm, slim.dropout], # 定义标志默认值\n",
    "                            is_training=is_training):\n",
    "          # 拿到最后一层的输出net和重要节点的字典表end_points\n",
    "            net, end_points = inception_v3_base(inputs, scope=scope) # 用定义好的函数构筑整个网络的卷积部分\n",
    "            nett=net\n",
    "          # Auxiliary logits作为辅助分类的节点，对分类结果预测有很大帮助\n",
    "    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n",
    "                          stride=1, padding='SAME'): # 将卷积、最大池化、平均池化步长设置为1\n",
    "        aux_logits = end_points['Mixed_6e'] # 通过end_points取到Mixed_6e\n",
    "        # end_points['Mixed_6e']  --> 17x17x768\n",
    "        with tf.variable_scope('AuxLogits'):\n",
    "            aux_logits = slim.avg_pool2d(\n",
    "                    aux_logits, [5, 5], stride=3, padding='VALID',\n",
    "                    scope='AvgPool_1a_5x5') #5x5x768\n",
    "\n",
    "            aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n",
    "                                   scope='Conv2d_1b_1x1') #5x5x128\n",
    "\n",
    "          # Shape of feature map before the final layer.\n",
    "            aux_logits = slim.conv2d(\n",
    "              aux_logits, 768, [5, 5],\n",
    "              weights_initializer=trunc_normal(0.01),\n",
    "              padding='VALID', scope='Conv2d_2a_5x5')  #1x1x768\n",
    "\n",
    "            aux_logits = slim.conv2d(\n",
    "              aux_logits, num_classes, [1, 1], activation_fn=None,\n",
    "              normalizer_fn=None, weights_initializer=trunc_normal(0.001),\n",
    "              scope='Conv2d_2b_1x1')  # 1*1*1000\n",
    "\n",
    "        if spatial_squeeze: # tf.squeeze消除tensor中前两个为1的维度。\n",
    "            aux_logits = tf.squeeze(aux_logits, [1, 2], name='SpatialSqueeze')\n",
    "            end_points['AuxLogits'] = aux_logits # 最后将辅助分类节点的输出aux_logits储存到字典表end_points中\n",
    "\n",
    "      # 处理正常的分类预测逻辑\n",
    "      # Final pooling and prediction\n",
    "    with tf.variable_scope('Logits'):\n",
    "        # net --> 8x8x2048\n",
    "        net = slim.avg_pool2d(net, [8, 8], padding='VALID',\n",
    "                              scope='AvgPool_1a_8x8') #1x1x2048\n",
    "\n",
    "        net = slim.dropout(net, keep_prob=dropout_keep_prob, scope='Dropout_1b')\n",
    "        end_points['PreLogits'] = net\n",
    "\n",
    "        # 激活函数和规范化函数设为空\n",
    "#         logits = slim.conv2d(net, num_classes, [1, 1], activation_fn=None,\n",
    "#                              normalizer_fn=None, scope='Conv2d_1c_1x1') # 1x1x1000\n",
    "        logits = slim.fully_connected(net, num_classes, activation_fn=None,\n",
    "                             normalizer_fn=None, scope='fc')\n",
    "        w_variables = slim.get_model_variables()[-2]\n",
    "        if spatial_squeeze: # tf.squeeze去除输出tensor中维度为1的节点\n",
    "            logits = tf.squeeze(logits, [1, 2], name='SpatialSqueeze')\n",
    "\n",
    "#     end_points['Logits'] = logits\n",
    "#     end_points['Predictions'] = prediction_fn(logits, scope='Predictions') # Softmax对结果进行分类预测\n",
    "    return logits , nett, w_variables # 最后返回logits和包含辅助节点的end_points\n",
    "\n",
    "\n",
    "# height, width = 299, 299 # 图片尺寸\n",
    "\n",
    "tf.reset_default_graph()\n",
    "is_training=True\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((299, 299, 3))\n",
    "y = neural_net_label_input(10)\n",
    "\n",
    "with slim.arg_scope(inception_v3_arg_scope()): # scope中包含了batch normalization默认参数，激活函数和参数初始化方式的默认值\n",
    "    logits,nett,ww = inception_v3(x, is_training=is_training) # inception_v3中传入inputs获取里logits和end_points\n",
    "\n",
    "logits = tf.identity(logits, name='logits')\n",
    "last_net = tf.identity(nett, name='last_net')\n",
    "last_w = tf.identity(ww, name='last_w')\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.005).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=8\n",
    "save_model_path = './model./model'\n",
    "saver = tf.train.Saver()\n",
    "save_loss=[]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):       \n",
    "        for batch_i in range(n_batches):\n",
    "            for batch_features, batch_labels in load_preprocess_training_batch(batch_i, batch_size):\n",
    "                sess.run(optimizer, feed_dict={\n",
    "                    x: batch_features,\n",
    "                    y: batch_labels})\n",
    "            if batch_i%10==0:\n",
    "                print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "                save_loss.append(print_stats(sess, batch_features, batch_labels, cost, accuracy))\n",
    "        print(epoch+1)\n",
    "        save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './model./model'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        valid_acc = sess.run(loaded_acc, feed_dict={\n",
    "        loaded_x: test_features,\n",
    "        loaded_y: test_labels})\n",
    "        print('Testing Accuracy: {}\\n'.format(valid_acc))       \n",
    "        \n",
    "        random_test_features=test_features[:n_samples]\n",
    "        random_test_labels=test_labels[:n_samples]\n",
    "\n",
    "        random_test_predictions = sess.run(\n",
    "                tf.nn.top_k(loaded_logits, top_n_predictions),\n",
    "                feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels})\n",
    "        \n",
    "        display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_model_path = './model./model'\n",
    "n_samples = 10\n",
    "\n",
    "def CAM_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "\n",
    "        loaded_w = loaded_graph.get_tensor_by_name('last_w:0')\n",
    "        loaded_net = loaded_graph.get_tensor_by_name('last_net:0')\n",
    "\n",
    "            \n",
    "        random_test_features=test_features[:n_samples]\n",
    "        random_test_labels=test_labels[:n_samples]\n",
    "\n",
    "        scores0 = sess.run(slim.softmax(loaded_logits),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels})\n",
    "        print(sess.run(tf.equal(tf.argmax(scores0, 1), tf.argmax(random_test_labels, 1))))\n",
    "\n",
    "        w=sess.run(loaded_w, feed_dict={\n",
    "                    loaded_x: random_test_features,\n",
    "                    loaded_y: random_test_labels})\n",
    "        net=sess.run(loaded_net, feed_dict={\n",
    "                    loaded_x: random_test_features,\n",
    "                    loaded_y: random_test_labels})\n",
    "        \n",
    "        print(net.shape)\n",
    "        print(w.shape)\n",
    "        for ij in range(len(random_test_labels)):\n",
    "            weights_LR = w\n",
    "            activation_lastconv = np.array([net[ij]])\n",
    "            weights_LR = weights_LR.T\n",
    "            activation_lastconv = activation_lastconv.T\n",
    "            \n",
    "            topNum = 1 # generate heatmap for top X prediction results\n",
    "            scores=scores0[ij]\n",
    "            scoresMean = np.mean(scores, axis=0)\n",
    "            ascending_order = np.argsort(scoresMean)\n",
    "            IDX_category = ascending_order[::-1] # [::-1] to sort in descending order\n",
    "\n",
    "            curCAMmapAll = py_returnCAMmap(activation_lastconv, weights_LR[[random_test_labels[ij].index(1)],:])\n",
    "            for kk in range(topNum):\n",
    "                curCAMmap_crops = curCAMmapAll[:,:,kk]\n",
    "                curCAMmapLarge_crops = cv2.resize(curCAMmap_crops, (256,256))\n",
    "                curHeatMap = cv2.resize(im2double(curCAMmapLarge_crops),(256,256)) # this line is not doing much\n",
    "                curHeatMap = im2double(curHeatMap)\n",
    "                curHeatMap = py_map2jpg(curHeatMap, None, 'jet')\n",
    "                image = cv2.resize(random_test_features[ij], (256, 256))\n",
    "                curHeatMap = im2double(image)*0.2+im2double(curHeatMap)*0.7\n",
    "\n",
    "                cv2.imshow('', curHeatMap)\n",
    "                cv2.waitKey(0)\n",
    "\n",
    "CAM_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
